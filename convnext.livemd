<!-- livebook:{"persist_outputs":true} -->

# facebook/convnext-tiny-224

```elixir
Mix.install(
  [
    {:bumblebee, "~> 0.3.0"},
    {:axon, "~> 0.5.1"},
    {:nx, "~> 0.5.1"},
    {:exla, "~> 0.5.1"},
    {:explorer, "~> 0.5.0"},
    {:kino_bumblebee, "~> 0.3.0"},
    {:stb_image, "~> 0.6"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Modify a spec

First of all we have to load a spec of the network to check out what's actually going on there

```elixir
{:ok, spec} = Bumblebee.load_spec({:hf, "facebook/convnext-tiny-224"})
```

<!-- livebook:{"output":true} -->

```
{:ok,
 %Bumblebee.Vision.ConvNext{
   architecture: :for_image_classification,
   num_channels: 3,
   patch_size: 4,
   hidden_sizes: [96, 192, 384, 768],
   depths: [3, 3, 9, 3],
   activation: :gelu,
   scale_initial_value: 1.0e-6,
   drop_path_rate: 0.0,
   layer_norm_epsilon: 1.0e-12,
   initializer_scale: 0.02,
   output_hidden_states: false,
   num_labels: 1000,
   id_to_label: %{
     912 => "worm fence, snake fence, snake-rail fence, Virginia fence",
     326 => "lycaenid, lycaenid butterfly",
     33 => "loggerhead, loggerhead turtle, Caretta caretta",
     289 => "snow leopard, ounce, Panthera uncia",
     518 => "crash helmet",
     990 => "buckeye, horse chestnut, conker",
     456 => "bow",
     401 => "accordion, piano accordion, squeeze box",
     168 => "redbone",
     309 => "bee",
     117 => "chambered nautilus, pearly nautilus, nautilus",
     377 => "marmoset",
     579 => "grand piano, grand",
     277 => "red fox, Vulpes vulpes",
     399 => "abaya",
     413 => "assault rifle, assault gun",
     477 => "carpenter's kit, tool kit",
     246 => "Great Dane",
     704 => "parking meter",
     175 => "otterhound, otter hound",
     740 => "power drill",
     621 => "lawn mower, mower",
     869 => "trench coat",
     219 => "cocker spaniel, English cocker spaniel, cocker",
     923 => "plate",
     492 => "chest",
     360 => "otter",
     669 => "mosquito net",
     12 => "house finch, linnet, Carpodacus mexicanus",
     192 => "cairn, cairn terrier",
     974 => "geyser",
     311 => "grasshopper, hopper",
     864 => "tow truck, tow car, wrecker",
     327 => "starfish, sea star",
     916 => "web site, website, internet site, site",
     ...
   }
 }}
```

Ok so we have a bunch of stuff here. I think the only interesting keys in this config are `num_labels` and `id_to_label`.

Right now there this network recongizes a wide set of classes. As we can read in the model [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/resnet) it is using _ImageNet_ class list - the url in the docs does not work, but I think this is this [list](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/)

So let's try to create our own labels

```elixir
id_to_label = %{
  0 => "beksinski",
  1 => "lowicz",
  2 => "duck",
  3 => "sfi",
  4 => "lisbon"
}

spec = Bumblebee.configure(spec, num_labels: 5, id_to_label: id_to_label)
```

<!-- livebook:{"output":true} -->

```
%Bumblebee.Vision.ConvNext{
  architecture: :for_image_classification,
  num_channels: 3,
  patch_size: 4,
  hidden_sizes: [96, 192, 384, 768],
  depths: [3, 3, 9, 3],
  activation: :gelu,
  scale_initial_value: 1.0e-6,
  drop_path_rate: 0.0,
  layer_norm_epsilon: 1.0e-12,
  initializer_scale: 0.02,
  output_hidden_states: false,
  num_labels: 5,
  id_to_label: %{0 => "beksinski", 1 => "lowicz", 2 => "duck", 3 => "sfi", 4 => "lisbon"}
}
```

## Load a model

Ok so now we have a tweaked spec of the network, now we need to actually load it

If you have no idea what featurizer is (like me), check it this video - it should help

https://www.youtube.com/watch?v=PBzGxFxMCuA&ab_channel=Rasa

```elixir
{:ok, model} = Bumblebee.load_model({:hf, "facebook/convnext-tiny-224"}, spec: spec)
{:ok, featurizer} = Bumblebee.load_featurizer({:hf, "facebook/convnext-tiny-224"})
```

<!-- livebook:{"output":true} -->

```

02:02:04.361 [debug] the following parameters were ignored, because of non-matching shape:

  * image_classification_head.output.kernel (expected {768, 5}, got: {768, 1000})
  * image_classification_head.output.bias (expected {5}, got: {1000})


```

<!-- livebook:{"output":true} -->

```
{:ok,
 %Bumblebee.Vision.ConvNextFeaturizer{
   resize: true,
   size: 224,
   resize_method: :bicubic,
   crop_percentage: 0.875,
   normalize: true,
   image_mean: [0.485, 0.456, 0.406],
   image_std: [0.229, 0.224, 0.225]
 }}
```

## Prepare a dataset

I've made 26 photos of each artifact (different lightning, camera angles), now we need to load them, split correctly to the data set and train set, featurize them and finally create batches

```elixir
defmodule Dataset do
  def load_and_split(path, opts \\ []) do
    {train_data, test_data} =
      Path.wildcard(path <> "*")
      |> Enum.map(fn artifact_path ->
        load_artifact_images(artifact_path, opts[:training_data_size])
      end)
      |> Enum.reduce({[], []}, fn {train_data, test_data}, {train_acc, test_acc} ->
        {train_data ++ train_acc, test_data ++ test_acc}
      end)

    {Enum.shuffle(train_data), Enum.shuffle(test_data)}
  end

  def batch(dataset, featurizer, opts \\ []) do
    dataset
    |> Enum.chunk_every(opts[:batch_size])
    |> Enum.map(fn batch ->
      {images, labels} = Enum.unzip(batch)
      featurized = Bumblebee.apply_featurizer(featurizer, images)

      {featurized, Nx.stack(labels)}
    end)
  end

  defp load_artifact_images(path, training_data_size) do
    Path.wildcard(path <> "/*.jpg")
    |> Enum.map(fn file_path -> load_and_tag(file_path) end)
    |> Enum.shuffle()
    |> Enum.split(training_data_size)
  end

  defp load_and_tag(file_path) do
    tag =
      file_path
      |> String.split("/")
      |> Enum.take(-2)
      |> List.first()
      |> String.to_integer()

    {:ok, file} = StbImage.read_file(file_path)

    {file, tag}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Dataset, <<70, 79, 82, 49, 0, 0, 17, ...>>, {:load_and_tag, 1}}
```

```elixir
path = "../seek-poc/dataset/"

{raw_train_data, raw_test_data} = Dataset.load_and_split(path, training_data_size: 20)

train_data = Dataset.batch(raw_train_data, featurizer, batch_size: 10)
test_data = Dataset.batch(raw_test_data, featurizer, batch_size: 10)

{length(train_data), length(test_data)}
```

<!-- livebook:{"output":true} -->

```
{10, 3}
```

## Train the model

We need to extract only some parts of a model

```elixir
%{model: cropped_model, params: params} = model

cropped_model
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"pixel_values" => {nil, 224, 224, 3}}
  outputs: "container_18"
  nodes: 164
>
```

```elixir
[{input, _}] = Enum.take(train_data, 1)
Axon.get_output_shape(cropped_model, input)
```

<!-- livebook:{"output":true} -->

```
%{hidden_states: #Axon.None<...>, logits: {10, 5}}
```

```elixir
logits_model = Axon.nx(cropped_model, & &1.logits)
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"pixel_values" => {nil, 224, 224, 3}}
  outputs: "nx_0"
  nodes: 165
>
```

Now we can construct a training loop. I have no idea what is the proper way of training such neural network so I've copied it from Bumblebee repo, played around with it and something like this is kind of working

```elixir
loss =
  &Axon.Losses.categorical_cross_entropy(&1, &2,
    reduction: :mean,
    from_logits: true,
    sparse: true
  )

optimizer = Axon.Optimizers.adam(5.0e-5)

accuracy = &Axon.Metrics.accuracy(&1, &2, from_logits: true, sparse: true)

trained_model_state =
  logits_model
  |> Axon.Loop.trainer(loss, optimizer, log: 1)
  |> Axon.Loop.metric(accuracy, "accuracy")
  |> Axon.Loop.checkpoint(event: :epoch_completed)
  |> Axon.Loop.run(train_data, params, epochs: 4, compiler: EXLA, strict?: false)

:ok
```

<!-- livebook:{"output":true} -->

```

02:03:47.695 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 9, accuracy: 0.7000000 loss: 1.3583120
Epoch: 1, Batch: 9, accuracy: 1.0000000 loss: 1.0621382
Epoch: 2, Batch: 9, accuracy: 1.0000000 loss: 0.8430476
Epoch: 3, Batch: 9, accuracy: 1.0000000 loss: 0.6820100
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Evaluating the model

```elixir
logits_model
|> Axon.Loop.evaluator()
|> Axon.Loop.metric(accuracy, "accuracy")
|> Axon.Loop.run(test_data, trained_model_state, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

02:07:20.117 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Batch: 2, accuracy: 1.0000000
```

<!-- livebook:{"output":true} -->

```
%{
  0 => %{
    "accuracy" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.815276337.2800615443.113429>
      1.0
    >
  }
}
```

```elixir
trained_model = Map.put(model, :params, trained_model_state)

serving =
  Bumblebee.Vision.image_classification(trained_model, featurizer,
    compile: [batch_size: 1],
    defn_options: [compiler: EXLA]
  )

image_input = Kino.Input.image("Image", size: {224, 224})
form = Kino.Control.form([image: image_input], submit: "Run")
frame = Kino.Frame.new()

Kino.listen(form, fn %{data: %{image: image}} ->
  if image do
    Kino.Frame.render(frame, Kino.Text.new("Running..."))

    image = image.data |> Nx.from_binary(:u8) |> Nx.reshape({image.height, image.width, 3})

    output = Nx.Serving.run(serving, image)

    output.predictions
    |> Enum.map(&{&1.label, &1.score})
    |> Kino.Bumblebee.ScoredList.new()
    |> then(&Kino.Frame.render(frame, &1))
  end
end)

Kino.Layout.grid([form, frame], boxed: true, gap: 16)
```
